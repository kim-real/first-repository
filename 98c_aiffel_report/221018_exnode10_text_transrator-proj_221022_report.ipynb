{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4f37c4",
   "metadata": {},
   "source": [
    "    Exnode 10. Text Transrator\n",
    "        1. package loading\n",
    "        2. data loading : \n",
    "            - reading \n",
    "            - drop & slice \n",
    "            - add\n",
    "        3. text vocabulary ( vocabulary ... incoding(text sequence > num sequence))\n",
    "            - english\n",
    "            - french\n",
    "            - vocabulary length > 변수         \n",
    "        4. 전처리 \n",
    "            - word max len\n",
    "            - 모델 decoder 생성(french 2개 필요\n",
    "            - padding \n",
    "            - one-hot incoding\n",
    "            - validation data creation \n",
    "        5. model training \n",
    "            - package loading\n",
    "            - incoder model ... hidden state , cell state creation \n",
    "            - decoder model .. input\n",
    "            - decoder model .. output\n",
    "            - model define & compile\n",
    "            - model fit\n",
    "        6. test model 생성\n",
    "            - encoder model\n",
    "            - decoder model\n",
    "            - output layer design\n",
    "            - word> num , num > word dictionary\n",
    "            - 모델 진행\n",
    "            - 문장 sample 입력 및 test\n",
    "        7. 결론 \n",
    "            - 모델 학습 전 전처리 부분에 많은 시간과 노력이 요구되었으며, 앞으로 진행하게 될 모든 학습에 전처리부문이 중요하게\n",
    "              진행될 것으로 예상됩니다.\n",
    "            - 모델학습등 모든 부분이 에러없이 진행되었으나 마지막 문장 해석부분에서 오류가 발생하였습니다. \n",
    "              5일간 고생하다가 node안에 고민하다 풀리지 않을 시 보라는 문장 예시를 보고 겨우 진행하였습니다. \n",
    "              이것을 하면서 심한 자괴감을 느꼈습니다.\n",
    "              그동안 node를 보면서 진행하는 수준으로 느꼈고, 지금까지의 학습이 소용이 없었다는 것에 2달동안 진행한 것이\n",
    "              겉핧기로 배운 것으로 생각되었습니다.\n",
    "              좀 더 모델 진행 및 차원 변화등에 많은 지식을 쌓아야겠습니다. \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eb983842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. package loading\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9e4027b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175437</th>\n",
       "      <td>I have an older brother and a younger sister.</td>\n",
       "      <td>J'ai un grand frère et une petite sœur.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163234</th>\n",
       "      <td>Muslims always pray facing toward Mecca.</td>\n",
       "      <td>Les musulmans prient toujours en faisant face ...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96829</th>\n",
       "      <td>It was a strange experience.</td>\n",
       "      <td>C'était une expérience étrange.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48048</th>\n",
       "      <td>Do I look like I care?</td>\n",
       "      <td>Ai-je l'air de m'en soucier ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104861</th>\n",
       "      <td>Please sit down for a moment.</td>\n",
       "      <td>Veuillez vous asseoir un moment.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  eng  \\\n",
       "175437  I have an older brother and a younger sister.   \n",
       "163234       Muslims always pray facing toward Mecca.   \n",
       "96829                    It was a strange experience.   \n",
       "48048                          Do I look like I care?   \n",
       "104861                  Please sit down for a moment.   \n",
       "\n",
       "                                                      fra  \\\n",
       "175437            J'ai un grand frère et une petite sœur.   \n",
       "163234  Les musulmans prient toujours en faisant face ...   \n",
       "96829                     C'était une expérience étrange.   \n",
       "48048                       Ai-je l'air de m'en soucier ?   \n",
       "104861                   Veuillez vous asseoir un moment.   \n",
       "\n",
       "                                                       cc  \n",
       "175437  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "163234  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "96829   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "48048   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "104861  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. data loading -- read \n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0db837ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21249</th>\n",
       "      <td>Tom painted that.</td>\n",
       "      <td>Tom a peint cela.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25267</th>\n",
       "      <td>Keep away from me.</td>\n",
       "      <td>Gardez vos distances.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11779</th>\n",
       "      <td>Tell us a joke.</td>\n",
       "      <td>Raconte-nous une blague.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6564</th>\n",
       "      <td>You're silly.</td>\n",
       "      <td>Vous êtes idiotes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Aim high.</td>\n",
       "      <td>Vise haut.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                       fra\n",
       "21249   Tom painted that.         Tom a peint cela.\n",
       "25267  Keep away from me.     Gardez vos distances.\n",
       "11779     Tell us a joke.  Raconte-nous une blague.\n",
       "6564        You're silly.        Vous êtes idiotes.\n",
       "447             Aim high.                Vise haut."
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. data loading -- drop & slice\n",
    "lines = lines[['eng', 'fra']][:33000] # 3.3만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bf874775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9823</th>\n",
       "      <td>Do it this way.</td>\n",
       "      <td>&lt;sos&gt; Faites-le de cette manière. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25437</th>\n",
       "      <td>My hands are cold.</td>\n",
       "      <td>&lt;sos&gt; J'ai les mains gelées. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23928</th>\n",
       "      <td>I hate rainy days.</td>\n",
       "      <td>&lt;sos&gt; Je déteste les journées pluvieuses. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21192</th>\n",
       "      <td>Tom left earlier.</td>\n",
       "      <td>&lt;sos&gt; Tom est parti tout à l'heure. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>I'm skinny.</td>\n",
       "      <td>&lt;sos&gt; Je suis maigrichon. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                                              fra\n",
       "9823      Do it this way.          <sos> Faites-le de cette manière. <eos>\n",
       "25437  My hands are cold.               <sos> J'ai les mains gelées. <eos>\n",
       "23928  I hate rainy days.  <sos> Je déteste les journées pluvieuses. <eos>\n",
       "21192   Tom left earlier.        <sos> Tom est parti tout à l'heure. <eos>\n",
       "2159          I'm skinny.                  <sos> Je suis maigrichon. <eos>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. data loading --- add\n",
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "lines.fra = lines.fra.apply(lambda x : '<sos> '+ x + ' <eos>')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bf8eaa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프랑스어 악센트(accent) 삭제\n",
    "def to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# 단어와 구두점 사이에 공백 추가.  # 악센트 제거 함수 호출\n",
    "def preprocess_sentence(sent):\n",
    "    sent = to_ascii(sent.lower())\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"sos+\", r\"<sos>\", sent)\n",
    "    sent = re.sub(r\"eos+\", r\"<eos>\", sent)\n",
    "    \n",
    "  # 다수 개의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "78dbb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "   \n",
    "    for i in range(len(lines)):\n",
    "             \n",
    "        # source 데이터 전처리\n",
    "        src_line = [w for w in preprocess_sentence(lines.eng[i]).split()]\n",
    "    \n",
    "        # target 데이터 전처리\n",
    "        tar_line = [w for w in preprocess_sentence(lines.fra[i]).split()]\n",
    "      \n",
    "        encoder_input.append(src_line)\n",
    "        decoder_input.append(tar_line)\n",
    "       \n",
    "    return encoder_input, decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "33c19cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!', '<eos>'], ['<sos>', 'marche', '.', '<eos>'], ['<sos>', 'en', 'route', '!', '<eos>'], ['<sos>', 'bouge', '!', '<eos>'], ['<sos>', 'salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = load_preprocessed_data()\n",
    "print('인코더의 입력 :',encoder[:5])\n",
    "print('디코더의 입력 :',decoder[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "73719aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28, 1], [28, 1], [28, 1]]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. text vocabulary ... english \n",
    "eng_tokenizer = Tokenizer(filters=\"\", lower=False)           #  Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(encoder)                          # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(encoder)       # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1a4e940a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33000"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "26889a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 72, 9, 2], [1, 203, 3, 2], [1, 25, 490, 9, 2]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. text vocabulary ... french \n",
    "fra_tokenizer = Tokenizer(filters=\"\", lower=False)   #  Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(decoder)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(decoder)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "36c4e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4672\n",
      "프랑스어 단어장의 크기 : 8137\n"
     ]
    }
   ],
   "source": [
    "#3. text vocabulary ... vocabulary length > 변수 \n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b618fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "#4. 전처리 ... word max len\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fce9380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4672\n",
      "프랑스어 단어장의 크기 : 8137\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "#4. 전처리 ... word max len 확인 \n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9308890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 전처리 ... 모델 decoder 생성\n",
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0187459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 72, 9], [1, 203, 3], [1, 25, 490, 9]]\n",
      "[[72, 9, 2], [203, 3, 2], [25, 490, 9, 2]]\n"
     ]
    }
   ],
   "source": [
    "#4. 전처리 ... 모델 decoder 생성 확인 \n",
    "# 디코더의 입력의 경우에는 숫자 12(< eos > 토큰)가 제거되었고, \n",
    "#디코더의 출력의 경우에는 숫자 11(< sos > 토큰)이 제거되었습니다.\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c751d3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "#4. 전처리 ... padding\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3b56d84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28  1  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#4. 전처리 ... padding ... 확인(#4. 전처리 ... padding)\n",
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "489c81e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [18129  4822  1295 ... 30970 15872  2506]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4cca0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c8da1f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (30000, 8)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (30000, 17)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (30000, 17)\n",
      "영어 학습데이터의 크기(shape) : (3000, 8)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (3000, 17)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (3000, 17)\n"
     ]
    }
   ],
   "source": [
    "#4. 전처리 ... 33000 중 3000건만 검증데이터로 삼고\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_test))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_test))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "698993a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳\n"
     ]
    }
   ],
   "source": [
    "# 5. model training ... package loading\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('⏳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ad70569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dc399a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. model training - incoder model ... hidden state , cell state creation \n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(eng_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f8e5a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. model training - decoder model\n",
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(fra_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c5c94cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  5. model training  - decoder model .. output layer\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cb1a58f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, None, 64)     299008      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 64)     520768      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "masking_8 (Masking)             (None, None, 64)     0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_9 (Masking)             (None, None, 64)     0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 64), (None,  33024       masking_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 64), ( 33024       masking_9[0][0]                  \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 8137)   528905      lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,414,729\n",
      "Trainable params: 1,414,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#5. model training - model define & compile\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "#model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")  기존\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "01578227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 14s 34ms/step - loss: 3.2491 - acc: 0.6342 - val_loss: 1.8840 - val_acc: 0.6428\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.7349 - acc: 0.7106 - val_loss: 1.6333 - val_acc: 0.7556\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.5665 - acc: 0.7564 - val_loss: 1.5196 - val_acc: 0.7607\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.4550 - acc: 0.7691 - val_loss: 1.4204 - val_acc: 0.7753\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.3586 - acc: 0.7793 - val_loss: 1.3347 - val_acc: 0.7874\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.2669 - acc: 0.7964 - val_loss: 1.2503 - val_acc: 0.8066\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.1845 - acc: 0.8109 - val_loss: 1.1843 - val_acc: 0.8164\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.1190 - acc: 0.8200 - val_loss: 1.1341 - val_acc: 0.8230\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.0604 - acc: 0.8283 - val_loss: 1.0871 - val_acc: 0.8297\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 1.0073 - acc: 0.8348 - val_loss: 1.0491 - val_acc: 0.8346\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.9612 - acc: 0.8401 - val_loss: 1.0173 - val_acc: 0.8376\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.9197 - acc: 0.8444 - val_loss: 0.9871 - val_acc: 0.8403\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.8811 - acc: 0.8486 - val_loss: 0.9618 - val_acc: 0.8432\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.8456 - acc: 0.8523 - val_loss: 0.9388 - val_acc: 0.8465\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.8122 - acc: 0.8558 - val_loss: 0.9165 - val_acc: 0.8485\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.7808 - acc: 0.8593 - val_loss: 0.8971 - val_acc: 0.8506\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.7515 - acc: 0.8624 - val_loss: 0.8781 - val_acc: 0.8526\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.7238 - acc: 0.8652 - val_loss: 0.8630 - val_acc: 0.8552\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.6976 - acc: 0.8679 - val_loss: 0.8491 - val_acc: 0.8554\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.6727 - acc: 0.8705 - val_loss: 0.8346 - val_acc: 0.8584\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.6492 - acc: 0.8730 - val_loss: 0.8235 - val_acc: 0.8584\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.6270 - acc: 0.8756 - val_loss: 0.8108 - val_acc: 0.8604\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.6060 - acc: 0.8783 - val_loss: 0.8002 - val_acc: 0.8613\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.5857 - acc: 0.8809 - val_loss: 0.7917 - val_acc: 0.8627\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.5667 - acc: 0.8832 - val_loss: 0.7871 - val_acc: 0.8631\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.5484 - acc: 0.8854 - val_loss: 0.7746 - val_acc: 0.8640\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.5305 - acc: 0.8878 - val_loss: 0.7680 - val_acc: 0.8652\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.5134 - acc: 0.8901 - val_loss: 0.7615 - val_acc: 0.8661\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.4964 - acc: 0.8925 - val_loss: 0.7558 - val_acc: 0.8666\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.4809 - acc: 0.8947 - val_loss: 0.7498 - val_acc: 0.8678\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.4655 - acc: 0.8971 - val_loss: 0.7427 - val_acc: 0.8692\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.4506 - acc: 0.8995 - val_loss: 0.7385 - val_acc: 0.8699\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.4367 - acc: 0.9019 - val_loss: 0.7355 - val_acc: 0.8714\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.4233 - acc: 0.9039 - val_loss: 0.7324 - val_acc: 0.8712\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.4102 - acc: 0.9063 - val_loss: 0.7278 - val_acc: 0.8716\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3978 - acc: 0.9085 - val_loss: 0.7237 - val_acc: 0.8730\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3859 - acc: 0.9104 - val_loss: 0.7205 - val_acc: 0.8738\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3749 - acc: 0.9124 - val_loss: 0.7200 - val_acc: 0.8738\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3639 - acc: 0.9146 - val_loss: 0.7171 - val_acc: 0.8753\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3532 - acc: 0.9166 - val_loss: 0.7116 - val_acc: 0.8747\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3431 - acc: 0.9185 - val_loss: 0.7128 - val_acc: 0.8753\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3341 - acc: 0.9204 - val_loss: 0.7107 - val_acc: 0.8763\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3248 - acc: 0.9220 - val_loss: 0.7086 - val_acc: 0.8772\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3164 - acc: 0.9239 - val_loss: 0.7092 - val_acc: 0.8768\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.3082 - acc: 0.9254 - val_loss: 0.7093 - val_acc: 0.8770\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.2996 - acc: 0.9271 - val_loss: 0.7088 - val_acc: 0.8772\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.2923 - acc: 0.9284 - val_loss: 0.7088 - val_acc: 0.8778\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.2850 - acc: 0.9302 - val_loss: 0.7065 - val_acc: 0.8781\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.2781 - acc: 0.9314 - val_loss: 0.7064 - val_acc: 0.8777\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.2716 - acc: 0.9325 - val_loss: 0.7067 - val_acc: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4e39b823a0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. model training - model fit\n",
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "66f0109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, None, 64)          299008    \n",
      "_________________________________________________________________\n",
      "masking_8 (Masking)          (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                [(None, 64), (None, 64),  33024     \n",
      "=================================================================\n",
      "Total params: 332,032\n",
      "Trainable params: 332,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#6 test model .. encoder_model \n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e78469b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 test model .. decoder_model \n",
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(64,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(64,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a105f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 64)     520768      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 64), ( 33024       embedding_10[1][0]               \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 8137)   528905      lstm_10[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,082,697\n",
      "Trainable params: 1,082,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#6 test model .. output layer 설계\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a9caa354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 test model ... 단어에서 정수로, 정수에서 단어로 바꾸는 사전(dictionary)을 준비\n",
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c19ee750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1)) \n",
    "    target_seq[0, 0] = fra2idx['<sos>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장     \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "08b8bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장:  Bouge ! \n",
      "번역기가 번역한 문장:  l endroit est mort\n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장:  Bonjour ! \n",
      "번역기가 번역한 문장:  vous avez ete parfait\n",
      "-----------------------------------\n",
      "입력 문장: Got it?\n",
      "정답 문장:  T'as capté ? \n",
      "번역기가 번역한 문장:  j ai couru a l etage\n",
      "-----------------------------------\n",
      "입력 문장: Hang on.\n",
      "정답 문장:  Tiens bon ! \n",
      "번역기가 번역한 문장:  quelqu un le choix\n",
      "-----------------------------------\n",
      "입력 문장: Here's $5.\n",
      "정답 문장:  Voilà cinq dollars. \n",
      "번역기가 번역한 문장:  viens encore dehors\n"
     ]
    }
   ],
   "source": [
    "#6 test model .. 출력결과 test\n",
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][5:len(lines.fra[seq_index])-5]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a1b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
